<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[混响时间的资料]]></title>
    <url>%2Ftech%2Faudio%2Frt-ref%2F</url>
    <content type="text"><![CDATA[-&gt; http://hyperphysics.phy-astr.gsu.edu/hbase/Acoustic/revtim.html Highlights $\text{RT}_{60}$: time to drop $60~\text{dB}$ below original level. For a general purpose auditorium for both speech and music: 1.5 to 2.5 seconds. Longer RT: richer musical sound, more difficulty understanding speech. Shorter RT: clearer speech, loss of richness and fullness. Reverberation time approximation: Sabine formula, $\text{RT}_{60} = 0.16 \frac{V}{S_e}$, $S_e$ is the effective absorbing area, $a$ is the absorption coefficient. $S_e=a_1S_1+a_2S_2+a_3S_3+\cdots$. -&gt; http://hyperphysics.phy-astr.gsu.edu/hbase/Acoustic/revmod.html#c2 Examples of reverberation times: Vienna, Musikvereinsaal: 2.05 seconds Boston, Symphony Hall: 1.8 seconds New York, Carnegie Hall: 1.7 seconds]]></content>
      <categories>
        <category>tech</category>
        <category>audio</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ARD simulator]]></title>
    <url>%2Ftech%2Faudio%2FARD-simulator%2F</url>
    <content type="text"><![CDATA[完成了一个基于 ARD 的三维声场仿真程序 -&gt; https://jinnsjj.github.io/ARD-simulator/]]></content>
      <categories>
        <category>tech</category>
        <category>audio</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[pre-defense]]></title>
    <url>%2Ftech%2Faudio%2Fpre-defense%2F</url>
    <content type="text"><![CDATA[sound space rendering based on the virtual sphere model pre-defense from JunjieShi3]]></content>
      <categories>
        <category>tech</category>
        <category>audio</category>
      </categories>
      <tags>
        <tag>spatial audio</tag>
        <tag>hoa</tag>
        <tag>room acoustics</tag>
        <tag>ard</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文笔记 - HOA 的必读论文 poletti2005]]></title>
    <url>%2Fnote%2Fpoletti2005%2F</url>
    <content type="text"><![CDATA[Three-dimensional surround sound systems based on spherical harmonics Poletti, M. A. (2005). Three-dimensional surround sound systems based on spherical harmonics. Journal of the Audio Engineering Society, 53(11), 1004-1025. IntroductionApproaches to 3D sound field reproduction The Kirchhoff–Helmholtz integral The Kirchhoff–Helmholtz integral shows that reproduction is possible inside a region if the pressure and normal velocity are known on the surface of the region. basis for the wave field synthesis (WFS). In practice simplifications are possible; for example, monopole sources are sufficient, and only those transducers in the direction of the sound source are required. Inverse method an inverse matrix is derived for a given geometry of loudspeakers and receiver positions, which allows the creation of the required sound pressure at a set of discrete points. 3D Ambisonics approach based on a spherical harmonic decomposition of the sound field This paper reviews and extends the theory of 3D sound systems based on spherical harmonics. Description of 3D sound fields这一章主要讲如何将录制声场并转化为球谐系数。 spherical harmonics/Spherical Bessel DescriptionFor the interior case, where all sources lie outside the region of interest: $$p(r,\theta,\phi,k)=\sum_{n=0}^{\infty}{\sum_{m=-n}^{n}{A_n^m(k)j_n(kr)Y_n^m}(\theta,\phi)}$$ $j_n(kr)$: the spherical Bessel function of the first kind. An important feature of this expansion is that for small $kr$, that is, for low frequencies or small distances from the origin, the summation in $n$ may be truncated to a finite value $N$ with little error, because only the low-order spherical Bessel functions have significant values for small $kr$. The pressure is then accurately represented by a total of $(N+1)^2$ terms. For the exterior case, where all sources lie within a region of space: $$p(r,\theta,\phi,k)=\sum_{n=0}^{\infty}{\sum_{m=-n}^{n}{B_n^m(k)h_n(kr)Y_n^m}(\theta,\phi)}$$ $h_n(kr)$: the spherical Hankel function. $h_n(kr)=j_n(kr)+iy_n(kr)$, $y_n(kr)$ is the spherical Bessel function of the second kind. The Wronskian relationship: $$j_n(x)h_n’(x)-j_n’(x)h_n(x)=\frac{i}{x^2}$$ Spherical harmonics form an orthonormal basis for any function defined on a sphere $$f(\theta,\phi)=\sum_{n=0}^{\infty}{\sum_{m=-n}^{n}{A_n^mY_n^m}(\theta,\phi)}$$ $$A_n^m = \int_0^{2\pi}{\int_0^{\pi}{f(\theta,\phi)Y_n^m(\theta,\phi)^*\sin(\theta)d\theta}d\phi}$$ Plane and spherical wave expansionsThe spherical harmonic expansion of a plane wave arriving from incidence angles $(\theta_i, \phi_i)$ is $$e^{i\boldsymbol{k_ir}}=4\pi\sum_{n=0}^{\infty}{i^nj_n(kr)\sum_{m=-n}^{n}{Y_n^m(\theta,\phi)Y_n^m(\theta_i,\phi_i)^*}}.$$ The spherical harmonic expansion of the wavefield due to a point source with position $(r_s, \theta_s, \phi_s)$, and with $r&lt;r_s$, is $$G(\boldsymbol{r}|\boldsymbol{r_s},k)=\frac{e^{ik|\boldsymbol{r}-\boldsymbol{r_s}|}}{4\pi|\boldsymbol{r}-\boldsymbol{r_s}|}=ik\sum_{n=0}^{\infty}{j_n(kr)h_n(kr_s)\sum_{m=-n}^{n}{Y_n^m(\theta,\phi)Y_n^m(\theta_i,\phi_i)^*}}.$$ Kirchhoff-Helmholtz descriptionThe Kirchhoff–Helmholtz integral shows that the sound pressure within an arbitrarily shaped volume of space may be calculated from the pressure and normal velocity on its surface $$p(\boldsymbol{r},k)=\int_S{\int{[ G(\boldsymbol{r}|\boldsymbol{r_s},k)\frac{\partial p(\boldsymbol{r_s},k)}{\partial n}-p(\boldsymbol{r_s},k)\frac{\partial G(\boldsymbol{r}|\boldsymbol{r_s},k)}{\partial n}]}dS}$$ This shows that the sound field may be reproduced from an infinite distribution of monopole sources excited by the normal gradient of the pressure on the surface ($G(\boldsymbol{r}|\boldsymbol{r_s},k)\frac{\partial p(\boldsymbol{r_s},k)}{\partial n}$), and an infinite distribution of dipole sources excited by the pressure on the surface ($p(\boldsymbol{r_s},k)\frac{\partial G(\boldsymbol{r}|\boldsymbol{r_s},k)}{\partial n}$). In practice, since most loudspeakers are monopoles at low frequencies, the dipole sources are less practical to implement (requiring, for example, unbaffled drivers). It has been shown that the monopole and dipole sources are not independent and that in practice the dipole sources may be ignored. 3D sound field recordingThe accurate recording of sound fields requires the synthesis of higher directivities than are available from first-order microphones. These directivities can be obtained using microphone arrays. array beamforming multiple discrete microphones the combination of dipole responses single with multiple radii A first-order microphone has response that is proportional to the pressure gradient, whereas a second-order microphone design has response proportional to the gradient of the gradient. Free-field sphere Decomposition$$A_n^m(k) = \frac{1}{j_n(kr)} \int_0^{2\pi}{\int_0^{\pi}{p(r,\theta,\phi,k)Y_n^m(\theta,\phi)^*\sin(\theta)d\theta}d\phi}$$ The zeros of $j_n(kr)$ produce equalization problems. One approach to removing the problem is to use first-order microphones facing radially outward. The general form of a radial first-order response is $$s_\alpha(r,\theta,\phi,k) = \alpha p(r,\theta,\phi,k)-(1-\alpha)\rho cv_R(r,\theta,\phi,k)$$ where $\alpha$ is the first-order parameter, $v_R$ is the radial velocity, and $\rho c$ is the impedance of free space. The first-order response then has the spherical harmonic expansion $$s_\alpha(r,\theta,\phi,k) = \sum_{n=0}^{\infty}{[\alpha j_n(kr)-\mathrm{i}(1-\alpha)j_n’(kr)]\sum_{m=-n}^{n}{A_n^m(k)Y_n^m}(\theta,\phi)}$$ and the spherical harmonic coefficients are $$A_n^m(k) = \frac{1}{\alpha j_n(kr)-\mathrm{i}(1-\alpha)j_n’(kr)} \int_0^{2\pi}{\int_0^{\pi}{s_\alpha(r,\theta,\phi,k) Y_n^m(\theta,\phi)^*\sin(\theta)d\theta}d\phi}$$ Solid sphere decompositionA solid sphere containing flush mounted pressure microphones also allows the coefficients to be found without the risk of zeros in the response. The scattering of sound around a sphere for a sound field is obtained by assuming that the resultant field is the sum of the original field and a scattered field that is radiating outward. $$p_t(r,\theta,\phi,k) = \sum_{n=0}^{\infty}{[j_n(kr)-\frac{j_n’(ka)}{h_n’(ka)}h_n(kr)]\sum_{m=-n}^{n}{A_n^m(k)Y_n^m}(\theta,\phi)}$$ where $a$ is the radius of the sphere. This is the sum of the original wavefield without the sphere and a scattering field consisting of outgoing waves whose coefficients are modified by a ratio of spherical Bessel terms. The sound field coefficients may be found from the pressure at $r = a$, $$A_n^m(k) = -\mathrm{i}(ka)^2h_n’(ka) \int_0^{2\pi}{\int_0^{\pi}{p_t(a,\theta,\phi,k) Y_n^m(\theta,\phi)^*\sin(\theta)d\theta}d\phi}$$]]></content>
      <categories>
        <category>note</category>
      </categories>
      <tags>
        <tag>sound field reproduction</tag>
        <tag>spatial audio</tag>
        <tag>spherical harmonics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matlab 音频低通滤波]]></title>
    <url>%2Ftech%2Faudio%2Fmatlab-audio-lowpass%2F</url>
    <content type="text"><![CDATA[用仿真生成的 transfer function，再高频部分精确度比较糟糕，所以希望只保留一定频率以下的部分，需要设计一个低通滤波器。这里介绍最为直接的办法，使用 matlab 里的 fir1 函数设计滤波器，并用 fftfilt 进行滤波。 1234b = fir1(n,fcut/(Fs/2)); % 第一个参数n是fir阶数，对应滤波器在时域的长度。 % 第二个参数是相对截止频率。 % 音频采样率Fs，对应的最高频率就是Fs/2，fcut是实际的截止频率。y = fftfilt(b,x) % 假设x是原始信号，用fftfilt对x进行滤波。 https://www.mathworks.com/help/signal/ref/fir1.htmlhttps://www.mathworks.com/help/signal/ref/fftfilt.html matlab中实际应用代码]]></content>
      <categories>
        <category>tech</category>
        <category>audio</category>
      </categories>
      <tags>
        <tag>matlab</tag>
        <tag>filter</tag>
        <tag>fir</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[冥想]]></title>
    <url>%2Fmeditation%2Fmeditation%2F</url>
    <content type="text"><![CDATA[我站在阳台冥想，想要了解生命的意义。远方的天空，一下泛蓝，一下泛红。但我猜那只是云后飞机的灯。附近河的流水吵吵闹闹。我想要抛开世界的噪音，洁净我的灵魂。我点燃一根烟，弄脏我的肺。]]></content>
      <categories>
        <category>meditation</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[linux 上的小玩具 cmatrix 和 cowsay]]></title>
    <url>%2Ftech%2Flinux-useless-toy%2F</url>
    <content type="text"><![CDATA[在 linux 上打发时间，发现两个无用但有趣的小玩具，cmatrix 和 cowsay。安装分别用 sudo apt-get install cmatrix 和 sudo apt-get install cowsay。 cmatrix 的效果是很俗的下字母雨。 cowboy 则是画一头牛，让它说一句话。 123456789$ cowsay 咩 ____&lt; 咩 &gt; ---- \ ^__^ \ (oo)\_______ (__)\ )\/\ ||----w | || || Have fun!]]></content>
      <categories>
        <category>tech</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>cmatrix</tag>
        <tag>cowsay</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Matlab 窗可视化工具 wvtool]]></title>
    <url>%2Ftech%2Fwvtool%2F</url>
    <content type="text"><![CDATA[通过matlab的 wvtool 可以看到不同窗的时域和频域特性，使用起来也十分方便。 1wvtool(hamming(64),hann(64),gausswin(64)) https://www.mathworks.com/help/signal/ref/wvtool.html]]></content>
      <categories>
        <category>tech</category>
      </categories>
      <tags>
        <tag>matlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Unity 读取资源文件 Resources.Load()]]></title>
    <url>%2Ftech%2FUnityResourceLoad%2F</url>
    <content type="text"><![CDATA[Unity 中使用 Resources.Load() 命令读取资源文件。 使用中有几个值得注意的地方： 读取文件时的根目录是 Assets/Resources，所有资源文件都放在该文件夹下，命令中的路径从 Resources 文件夹里开始写。 用 / 表示子文件夹。 读取的文件不要加文件的后缀。 Load 后 &lt;&gt; 中写入读取的类型。 举个例子，如果我们想要读取音频文件 Assets/Resources/AudioClips/1.wav，那么命令写作 12string fname = &quot;AudioClips/1&quot;;AudioClip clip = Resources.Load&lt;AudioClip&gt;(fname); 又比如想要读取一个混音文件 Assets/Resources/AudioMixerGroup.mixer，同上写作 12string fname = &quot;AudioMixerGroup&quot;;AudioMixer mixer = Resources.Load&lt;AudioMixer&gt;(fname); https://docs.unity3d.com/ScriptReference/Resources.Load.html]]></content>
      <categories>
        <category>tech</category>
      </categories>
      <tags>
        <tag>unity</tag>
        <tag>C#</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[librosa.load() 读取音频的采样率处理]]></title>
    <url>%2Ftech%2Faudio%2Flibrosa-load%2F</url>
    <content type="text"><![CDATA[利用python中的 librosa.load() 我们可以轻松的读取音频文件，但对于不同采样率的音频文件，使用这一函数有一些细节还需注意。 如果 sr 缺省，librosa会默认以22050的采样率读取音频文件，高于该采样率的音频文件会被下采样，低于该采样率的文件会被上采样。 如果希望以原始采样率读取音频文件，sr 应当设为 None。具体做法为 y, sr = librosa(filename, sr=None)。 https://librosa.github.io/librosa/generated/librosa.core.load.html]]></content>
      <categories>
        <category>tech</category>
        <category>audio</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>librosa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[音频重采样 python+librosa]]></title>
    <url>%2Ftech%2Faudio%2Fresample-audio%2F</url>
    <content type="text"><![CDATA[python中的 librosa.resample() 让我们可以非常方便的对音频文件进行重采样。 1234567891011import librosa# to install librosa package# &gt; conda install -c conda-forge librosa filename = 'ClapSound.wav'newFilename = 'ClapSound_8k.wav'y, sr = librosa.load(filename, sr=48000)y_8k = librosa.resample(y,sr,8000)librosa.output.write_wav(newFilename, y_8k, 8000) 目标是一个48kHz的音频 Your browser does not support the audio element. 利用librosa库中中的resample将这段音频下采样到8kHz。 Your browser does not support the audio element. 对应的 jupyter lab]]></content>
      <categories>
        <category>tech</category>
        <category>audio</category>
      </categories>
      <tags>
        <tag>signal processing</tag>
        <tag>python</tag>
        <tag>librosa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[impulse response 使用的踩坑 —— cconv]]></title>
    <url>%2Ftech%2Faudio%2Fcconv%2F</url>
    <content type="text"><![CDATA[在 impulse response 的使用中遇到了小小的问题，具体如图，第一行是原始信号，第二行是响应的信号，在通过将原始信号和相应信号分别转换到频域求得 transfer function 后，ifft 得到 impulse response。第三行是原始信号和 ir 进行卷积得到的结果，可以看到在信号最开始的阶段，有一个意料外的响应。： 这一段的代码如下： 123456789% src: 原始信号% resp: 响应的信号Ssrc = fft(src);Sresp = fft(rest);Stf = Ssrc./Sresp;ir = ifft(Stf);z = conv(src,ir); 先说短的结论，把最后一条命令 conv 换为 cconv 即可。 和 Daniel 探讨后，问题出在 得到的 ir 上。 对于这个ir，是不能直接进行卷积的，而应该先做一个 circular shift ( ir = circshift(ir, Nshift), z = circshift(z, -Nshift) )。让响应变得完整。 这样得到的响应就没有前面恼人的部分了。然而。。。。又出现了另一个问题，响应的后半部分消失了。。。。。 最后就索性使用圆周卷积，结果完美。 123456789% src: 原始信号% resp: 响应的信号Ssrc = fft(src);Sresp = fft(rest);Stf = Ssrc./Sresp;ir = ifft(Stf);z = cconv(src,ir,800); % 800因为信号的长度为800 看来，信号处理的基本功还是不太扎实啊。 圆周卷积与线性卷积的区别 Linear and Circular Convolution - MathWorks DocumentsCircular/Linear Convolution 与 DFT - CSDN blog 频域相乘等于时域卷积仅适用于圆周卷积。 对于长度N与长度L的向量进行线性卷积，结果长度为N+L-1。 对于圆周卷积，两个向量长度应当相同，不同的话要进行补零。 将进酒李白君不见，黄河之水天上来，奔流到海不复回。君不见，高堂明镜悲白发，朝如青丝暮成雪。人生得意须尽欢，莫使金樽空对月。天生我材必有用，千金散尽还复来。烹羊宰牛且为乐，会须一饮三百杯。岑夫子，丹丘生，将进酒，杯莫停。与君歌一曲，请君为我倾耳听。钟鼓馔玉不足贵，但愿长醉不复醒。古来圣贤皆寂寞，惟有饮者留其名。陈王昔时宴平乐，斗酒十千恣欢谑。主人何为言少钱，径须沽取对君酌。五花马，千金裘，呼儿将出换美酒，与尔同销万古愁。]]></content>
      <categories>
        <category>tech</category>
        <category>audio</category>
      </categories>
      <tags>
        <tag>impulse response</tag>
        <tag>signal processing</tag>
        <tag>matlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spatially oriented format for acoustics]]></title>
    <url>%2Ftech%2Faudio%2Fspatially-oriented-format-for-acoustics%2F</url>
    <content type="text"><![CDATA[UNDER CONSTRUCTION… SOFA (spatially oriented format for acoustics) 是一种存储空间声学数据的格式，在保存HRTF和BRIR这样的空间传播函数上格外方便。尤记得从前，使用HRTF，方向信息距离信息，都得写在文件名里，有了SOFA格式，这些信息会保存在文件中，使用起来会方便很多。 General specificationsObjects Name Description Receiver: 指各种声音的接受器，比如麦克风，或者HRTF里的人耳。receiver的个数会影响保存数据矩阵的size，SOFA中没有限制receiver的个数。 Listener: Receiver的载体，在HRTF中，Listener是人头。在DRIR中，listener是收音的麦克风阵列。Listener这个object可以将相关的一组receiver当作一个整体处理。AES69中只能有一个Listener。 Emitter: 指在发声的声源。数量没有限制。 Source: Emitter的载体，和emitter的关系如同Listener与receiver。AES69中只能有一个Source。 Room: 指在测量时候的空间模型，比如在很多场景下，room是free-field。AES69中只能有一个Room。 Objects 间关系坐标SOFA中有一个全局坐标系global coordinate system。listener和source的位置会记录在这个全局坐标系中。而listener和source分别拥有一个local coordinate system。记录receiver/emitter和listener/source的相对位置。global和coordinate坐标系都可以是Cartesian或者Spherical。 方向这里的方向是指local coordinate system的朝向，用“view”和“up”两个向量定义。根据自己的理解，对于freefield的HRTF这点不是很重要，但对于BRIR，listener的朝向就要靠这个来定义了。 Numerical container]]></content>
      <categories>
        <category>tech</category>
        <category>audio</category>
      </categories>
      <tags>
        <tag>spatial audio</tag>
        <tag>HRTF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[语音情感识别探讨]]></title>
    <url>%2Ftech%2Faudio%2Fspeech-emotion-recognition-survey%2F</url>
    <content type="text"><![CDATA[El Ayadi M, Kamel M S, Karray F. Survey on speech emotion recognition: Features, classification schemes, and databases[J]. Pattern Recognition, 2011, 44(3): 572-587. 简介语音识别的发展可以说是非常成熟，但距离我们的目标——自然的人机交互，还差的很远，这其中的一个原因就是现在机器还无法理解我们说话时的情感。这是研究语音情感识别的一个重要的motivation。 情感识别的用处 自然的人机交互（natural man-machine interaction）。 车载系统（in-car board system）。用于检测司机的精神状况以确保驾驶安全。 。。。 情感识别面临的挑战 什么样的特征在分辨情感中最有用。 一个发音中可以会包含多种感情，不同情感的边界也难以界定，哪个情绪是当前主导的情绪？ 表达情感是一个个性化极强的事情，根据个人，环境甚至文化差异都很大。 情绪可能持续很长时间，但期间也会有快速变化的情绪，情感识别系统是检测长期的情绪还是短时的情绪了（比如被炒鱿鱼了，会悲伤很久，但这期间吃了顿好吃的饭，虽然会开心，但人还处在伤心的状态中，那么该判定为悲伤还是开心呢）。 情感本身都难以明确的定义。 情感虽然情感本身十分复杂，但一个被广泛认同的模型是讲情感划分为两个维度：activation 和 valence。activation指得是表达这个情感需要的能量。比较强烈的情感比如愤怒，喜悦，恐惧。伴随着这类感情可能会有心跳加速，血压升高等等，同时人的语速会变快，音高变高。相反比较舒缓的请看比如忧伤，语速可能会降低，高频会减少。而activation类似的感情，比如愤怒与喜悦，则用valence来加以区分。用何种feature来描述valence尚无定论。因此，在情感识别系统中，强烈的感情与舒缓的感情很好区分，而区分不同类别的情感则还是一个挑战。 语音情感识别中的特征特征提取是模式识别任务中最重要的一个环节之一，在语音情感识别的任务中亦然。我们会面对四个主要的问题 特征提取的作用域。是对音频进行分帧(frame)再提取特征，或是对全局进行提取？ 提取什么样的特征？ 是否要进行前处理与后处理？比如移除静音的部分。 要不要结合其他的特征？比如语言模型或者面部表情。 对以上四个问题的分析 局部特征还是全局特征 全局特征在分类的准确率上往往比局部特征表现的要好，同时耗时也更少（特征量较少）。然而全局特征也有许多缺点： 只在分类高兴奋度的情感（high-arousal emotions，也是我们之前说的activation较高的情感）中比较有效，比如在分类anger和joy时，全局特征就会失效。 全局特征会丢失语音的短时信息（temporal information）。 当使用较为复杂的分类器（HMM，SVM等）时，全局变量会因为特征较少而无法进行有效的训练。 因此在复杂的模型中使用局部特征，模型的准确率更好。 还有一种做法是对语音信号根据音素进行分段而不是分帧。研究显示了把分段的特征和全局特征相结合可以一定程度提高是别的准确率。 提取什么样的特征 我们可以把语音特征划分为四类： Continuous speech features 连续语音特征 pitch-related features formants features energy-related features timing features articulation features常用的有F0，Energy，Duration，Formants。另外在特征的提取中，除了使用特征还对特征进行一些转换，比如平均，最大最小等。在INTERSPEECH 2009 中有个图表就很清晰的展示了这一点。对于每一帧信号，我们提取16个特征和它们的delta，并对这32个特征进行右边的12种变换，得到384维（(16x2)x12）的特征向量（每一帧）。 Voice quality features voice quality harsh tense breathy Spectral-based speech features LPC MFCC LFPC TEO-based features 小结：Continuous speech features 用来检测high-arousal和low-arousal的情感；频谱特征比如MFCC用来做N-way classification的问题，TEO-based features 用于压力检测； 语音处理 前处理 pre-emphasis filter, $H(z)=1-0.97z^{-1}$: to equalize the effect of the propagation of speech throungh air. overlapped frames: to smooth the extracted contours. Hamming window: to reduce ripples in the spectrum of the speech spectrum. slient intervals: 语音中的静音间隔也包含情感信息，通常会保留下来。 特征提取 后处理 正规化 feature normalization $\hat{x}=\frac{x-\mu}{\sigma}$ 重要！但由于方差中包含许多情感信息，normalize后会消除这些特征，要考虑到这一点。 降维 feature selection：找到分类效果最好的子特征。 feature extraction：对原始特征进行mapping到另一空间，从而达到降维效果。 声学特征与其他特征结合 语言信息（linguistic information） 视频信息 分类方法都是些大家熟悉的手法。 HMM：效果好（在语音情感识别的任务里，正确率甚至可能超过人类） GMM：比HMM高效，但不能利用短时特征 Neural networks：emmm。。大家都在用 SVM Multiple classifer system 这个比较有趣，可以把情感识别分几步来做，比如先分类 high arousal 和 low arousal 的情感，再进行子类别的分类可以看这个图。]]></content>
      <categories>
        <category>tech</category>
        <category>audio</category>
      </categories>
      <tags>
        <tag>speech emotion recognition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HOA声场重建原理]]></title>
    <url>%2Ftech%2Faudio%2Fsomething-of-HOA%2F</url>
    <content type="text"><![CDATA[空间音频是什么？空间音频很多地方也叫做三维音频，它源于我们人据有分辨声音来源方向的能力。虽然一直是比较冷门的研究方向，但在VR的发展下，空间音频也火了一把，很大程度是因为空间音频在在VR的应用所讲究的沉浸感中扮演着极为重要的作用，当声音的方向与画面不匹配的时候，沉浸感会荡然无存，所以高品质的空间音频也成为了VR研究中很重要的一环。 https://developers.google.com/resonance-audio/discover/overview HOA?HOA 全称是 Higher Order Ambisonics，强硬的翻译的话就是高次混响。它最初的目的是要重建空间中声场的分布。我们可以想象一个空间中的一个球面，我们在球的中心，那么从球外传来的声音会在这个球面上有一个投影，这让我们想到，我们可以把球面以外的声音都无视掉，并假设声源分布在这个球面上。用球面上的声源产生的声场来拟合原来生源产生的声场。HOA就是一个这样去拟合声场的方法。 https://developers.google.com/resonance-audio/discover/concepts 相关知识球面调和函数 Spherical harmonics球面调和函数也叫球谐函数，它并不好理解，有机会可以详细的聊一聊，这里只做最最最抽象的介绍，力求知其然不求知其所以然。 这里要做的是类比一下傅里叶变换。我们通常接触的函数都是分布在一维的，每一个 $x$ 对应一个 $f(x)$，我们对 $f(x)$ 进行傅里叶变换会得到 $F(\omega)$。现在我们的目标是一个在球面上分布的函数，我们使用一个球坐标系，球面上的点 $(r,\theta,\phi)$，而对于一个固定大小的球，$r$是一个定值，我们可以暂时只关注$\theta$和$\phi$，那么对于每一组 $(\theta,\phi)$对应一个$f(\theta,\phi)$。傅里叶变换可以将时域信号转化到频域，这样方便我们分析特定频域的信号，对于 $f(\theta,\phi)$，我们只想知道这个函数在空间中一个大概的分布，所以$f(\theta,\phi)$记录的东西无疑太多了，所以我们可以把它转换为 $F_n^m$，这里的 $n$ 和 $m$ 就类似与傅里叶中的 $\omega$。当我们只需要粗略的空间分辨率时，很小的$n$就足够，当需要更精细的空间分辨率时，就需要较大的 $n$ 时的 $F_n^m$，它们会对小的$n$的$F_n^m$的空间的缝隙中进行插值，使空间的分辨率增高。通过球谐分解，我们可以把球面上的函数用更少的数值表达。就如下面这张图，从上至下$n$逐渐增大，对空间的描述能力也逐渐增强。 https://en.wikipedia.org/wiki/Spherical_harmonics 基于球谐函数，球面上的任意函数可以被分解为球谐系数： $$p(\theta,\phi) = \sum_{n=0}^\infty \sum_{m=-n}^{n} P_n^m Y_n^m(\theta,\phi) $$ 声场中方向的信息都被转换到了球谐因子 $Y_n^m(\theta,\phi)$ 中，这个式子看起来是不是也非常像傅里叶变换呢。 球谐贝塞尔函数&amp;汉克尔函数 Spherical Bessel function, Spherical Hanker function这两个函数的表达式也比较复杂，好在我们通常也不需要记住，只需要知道它门描述了波动方程在球坐标系下的解，它与三个参数有关，对应的上面的球谐函数的阶数 $n$，波的频率球面的半径有关，在之后的数学中我们会用到。贝塞尔函数写作 $j_n(kr)$ 或者 $j_n(\frac{\omega}{c}r)$，$k$是一个波的波数，它等于 $\frac{\omega}{c}$，$\omega$ 表示声音的角频率，$r$表示球面的半径。汉克尔函数类似，但有一类汉克尔函数和二类汉克尔函数，分别表示从内往外传递的出射波和从外往内传播的入射波，写作 $h_n^{(1)}(kr)$ 与 $h_n^{(2)}(kr)$。 格林函数 Green function格林函数用于描述在开放空间中（没有障碍物没有反射折射）一个声源到空间另外一点的响应。用$\boldsymbol{x_0}$表示声源的位置，声源到$\boldsymbol{x}$点的传达函数为为 $G(\boldsymbol{x}-\boldsymbol{x_0},\omega) = \dfrac{e^{ik|\boldsymbol{x}-\boldsymbol{x_0}|}}{4\pi|\boldsymbol{x}-\boldsymbol{x_0}|}$。$\omega$表示声音的角频率，格林函数同样有在时域上的表达，可以看作是一个延时函数。 HOA!我们之前说到了，HOA的思路是用一个球面上的音源去拟合球内的声场。我们把这个思想在球坐标系中用公式描述出来。对于一个特定频率的声波（角频率为 $\omega$ ）球内一点 $\boldsymbol{x}$，该点的声强是球面$S_0$上声源的在该点的响应的叠加： $$p(\boldsymbol{x},\omega) = \int_{\Omega_0 \in S_0} D(\boldsymbol{x_0},\omega) G(\boldsymbol{x}-\boldsymbol{x_0},\omega)d\Omega_0$$ 这里 $D(\boldsymbol{x_0},\omega)$ 表示了球面分布的声源的驱动信号(driving signal)，$\boldsymbol{x_0}$ 则代表着它们对应的坐标。 于此同时，我们也可以对 $p(\boldsymbol{x},\omega)$ 进行球谐分解，把它写作： $$p(r,\theta,\phi,\omega) = \sum_{n=0}^\infty \sum_{m=-n}^{n} j_n(\frac{\omega}{c}r) P_n^m(\omega) Y_n^m(\theta,\phi)$$ 这一步，我们也叫做声场的编码，$p(\boldsymbol{x},\omega)$可以是录音得到的，也可以是仿真生成的。 因为$j_n(\frac{\omega}{c}r)$可能等于0，在某些频率无法得到$P_n^m(\omega)$,这时候就会遇到禁止频率的问题。这也是声场重建中一个比较重要的问题。 同样 $G(\boldsymbol{x}-\boldsymbol{x_0},\omega)$ 也进行对应的球谐变换。如果我们用$L$个扬声器重建声场，那么上面这个式子可以写作 $$p(r,\theta,\phi,\omega) = \sum_{n=0}^\infty \sum_{m=-n}^{n} j_n(\frac{\omega}{c}r) { \sum_{\ell=1}^{L} D_\ell(\boldsymbol{x_0},\omega)G_{nm}^{\ell}(\omega) } Y_n^m(\theta,\phi)$$ $$P_n^m = \boldsymbol{C(\omega)} \cdot \boldsymbol{D(\boldsymbol x_0,\omega)}$$ $$\boldsymbol{C(\omega)} = [G_{nm}^{1}(\omega) \quad G_{nm}^{2}(\omega) \quad \cdots \quad G_{nm}^{L}(\omega)]$$ 这样通过求矩阵的逆，我们就可以求得驱动信号$D(\boldsymbol x_0,\omega)$，从而重建声场。这也是HOA的解码。 同时我们也可以避免矩阵的求逆，直接算出驱动函数的解析解。这里就不展开了。]]></content>
      <categories>
        <category>tech</category>
        <category>audio</category>
      </categories>
      <tags>
        <tag>sound field reproduction</tag>
        <tag>spatial audio</tag>
        <tag>HOA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在hexo-NexT中插入note提示块]]></title>
    <url>%2Funcategorized%2Fhexo-next-note%2F</url>
    <content type="text"><![CDATA[hexo中可以插入一些很酷的提示块，效果如下： default 提示块标签 primary 提示块标签 success 提示块标签 info 提示块标签 warning 提示块标签 danger 提示块标签 使用方法1234567891011121314151617181920212223&#123;% note default %&#125;default 提示块标签&#123;% endnote %&#125;&#123;% note primary %&#125;primary 提示块标签&#123;% endnote %&#125;&#123;% note success %&#125;success 提示块标签&#123;% endnote %&#125;&#123;% note info %&#125;info 提示块标签&#123;% endnote %&#125;&#123;% note warning %&#125;warning 提示块标签&#123;% endnote %&#125;&#123;% note danger %&#125;danger 提示块标签&#123;% endnote %&#125; 主题配置文件中可以修改风格12345678910111213# Note tag (bs-callout).note: # Note tag style values: # - simple bs-callout old alert style. Default. # - modern bs-callout new (v2-v3) alert style. # - flat flat callout style with background, like on Mozilla or StackOverflow. # - disabled disable all CSS styles import of note tag. style: flat icons: true border_radius: 3 # Offset lighter of background in % for modern and flat styles (modern: -12 | 12; flat: -18 | 6). # Offset also applied to label tag variables. This option can work with disabled note tag. light_bg_offset: 0 除此之外还可以实现选项卡等很酷的功能。 tab 1tab 2A选项卡 1 选项卡 2 选项卡 3 名字为A 使用方法1234567891011&#123;% tabs tab,1 %&#125; 名字为tab，默认在第1个选项卡，如果是-1则隐藏&lt;!-- tab --&gt;**选项卡 1** &lt;!-- endtab --&gt;&lt;!-- tab --&gt;**选项卡 2**&lt;!-- endtab --&gt;&lt;!-- tab A --&gt;**选项卡 3** 名字为A&lt;!-- endtab --&gt;&#123;% endtabs %&#125; 参考→打造个性超赞博客Hexo+NexT+GithubPages的超深度优化Hexo+markdown优雅写博客]]></content>
      <tags>
        <tag>hexo</tag>
        <tag>NexT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[librosa音频操作基础和特征提取]]></title>
    <url>%2Ftech%2Faudio%2Flibrosa-audio%2F</url>
    <content type="text"><![CDATA[把项目页面里面的这个python音频操作和特征提取小教程搬到了这里，用gist分享，jupyter notebook也可以显示啦。 →更大更干净的页面 诉衷情·寒食仲殊涌金门外小瀛洲。寒食更风流。红船满湖歌吹，花外有高楼。晴日暖，淡烟浮。恣嬉游。三千粉黛，十二阑干，一片云头。]]></content>
      <categories>
        <category>tech</category>
        <category>audio</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>librosa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建hexo+next博客并把项目主页放在上面]]></title>
    <url>%2Funcategorized%2Fbuild-projects-page%2F</url>
    <content type="text"><![CDATA[搭建hexo+next博客并把项目主页放在上面其实博客还是想用csdn写，但是像jupyter notebook这些，想在网上贴出来，一方面和大家分享，一方面也想留作存档，在csdn就不太方便，还是githube page好。之前在github page上搭了博客，随后由于懒惰废弃了，想从新捡起，可当时安装了很多依赖，现在找不回来，导致hexo已经生成不了，因为之前用的不多，所以索性重新再来，也一边在这里记录，免得下次又忘。 这次还是选择hexo，一方面好看，另外也方便，配置好了后直接hexo d就可以同步到page上。 那么开始。 怎么在github pages上搭主页可以参考GitHub-Pages-Hexo-NexT-快速构建静态Blog。 找好看的主题首先要找个好的hexo主题，要明确我不是想用来做博客，而是分享一些项目，所以要求主页清晰明了。 找了一圈，没有特别满意的， 还是用next吧 主要步骤 装npm 装hexo 初始化hexo 命令行输入hexo init page page是想初始化的文件夹名 next主题 https://github.com/theme-next/hexo-theme-next 进入文件夹，克隆next库 cd page git clone https://github.com/theme-next/hexo-theme-next themes/next 切换主题：在根目录里的_config.yml文件中，theme字段设为next 完工，看一看完成情况hexo s 配置deploy https://hexo.io/docs/deployment.html 安装hexo-deployer-git 根目录里的_config.yml文件 12345deploy: type: git repo: git@github.com:username/username.github.io.git branch: master message: &quot;&#123;&#123; now(&apos;YYYY-MM-DD HH:mm:ss&apos;) &#125;&#125;&quot; hexo d部署 访问一下 jinnsjj.github.com 一些配置首先还是去_config.yml里，把最上面的title啊描述啊按自己的喜好改一改 资源文件夹对于每个文章，生成一个文件夹存放里面的媒体文件。 根目录里的_config.yml文件 1post_asset_folder: true 网站内搜索https://github.com/theme-next/hexo-generator-searchdb npm install hexo-generator-searchdb --save 站点的配置文件中加入 12345search: path: search.xml field: all format: html limit: 10000 注意field改为all，这样我们自己的project页面才能被搜索到。 leancloud阅读计数https://leaferx.online/2018/02/11/lc-security/ add this 分享https://theme-next.iissnan.com/third-party-services.html+https://github.com/iissnan/hexo-theme-next/issues/906 这一块可能会被chrome的ghost插件拦截，导致看不到效果。 放置项目的地方在source文件夹里面建立一个projects文件夹，把jupyter notebook导出的html文件或者其他的html直接放在里面就好。再在根目录的_config.yml中间的skip_render项中加入projects/**，这样这个文件夹里的html就会直接显示了。**指跳过所有文件与文件夹 *指跳过所有文件。 如何不处理source目录下某个子目录的所有文件，仅仅是将其copy到public目录中对应目录？ #1146 摄影作品展示的模板-riddle 无关紧要但好玩的东西主页上的黑猫:https://huaji8.top/post/live2d-plugin-2.0/https://github.com/EYHN/hexo-helper-live2d/blob/master/README.zh-CN.md 首先安装模块npm install --save hexo-helper-live2d _config.yml中加入1234567891011121314live2d: enable: true scriptFrom: local pluginRootPath: live2dw/ pluginJsPath: lib/ pluginModelPath: assets/ model: use: live2d-widget-model-hijiki display: position: right width: 150 height: 300 mobile: show: true model下use的模型需要安装npm install live2d-widget-model-hijiki 备注npm命令的–save是将安装的依赖包写入目录里的package.json文件中]]></content>
      <tags>
        <tag>hexo</tag>
        <tag>NexT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-37 Sudoku Solver]]></title>
    <url>%2Fcoding%2FSudoku-Solver%2F</url>
    <content type="text"><![CDATA[37. Sudoku Solver求解数独。前两天做了验证数独那道题，当时对于遍历hash的使用有了一些感悟，但看这道题时候特别头疼。这两天HackerRank也刷了一下，对于DFS和BFS的理解加深了一些，渐渐的思路就变得清晰。其实是很暴力的解法，但把他转化为代码，对几天前的我来说还有一些难度。经过这两天的学习，总算是可以勉强完成，但参考别人的答案后，果然进步空间还比较大。 我之前的版本，dfs函数没有加入i与j两个变量，每次调用都重新扫描一遍需要填的空，这导致效率很低，引入i和j后果然好了很多。另外之前的dfs是void不返回值，通过一个cnt变量计算总共的空数，当cnt等于0时停止，但由于我们需要在中途中断dfs函数（找到正确的解时，否则答案会被填回’.’），采用void的类型也会比较难以中断。借鉴了别人的解法，让dfs返回一个值来判定是否完成整个数独，这样好了很多。 算法解释： solveSudoku： 功能函数入口。 dfs：深度优先搜索函数。 输入整个表和一个坐标。 如果该坐标已有数字，搜索下一个格子j++；当j==9，说明这一排已搜索完，搜索下一排的第一个格子i++, j=0；如果所有的行都已搜索完，说明所有的格子都被正确的填入了数字，则返回true。 如果该坐标下没有数字&#39;.&#39;，从1开始填入，并验证该数字是否可行isValid(board, i, j)。如果不可行，擦除填入的数字并填入下一个数字直到9。如果可行，搜索下一个格子，并直到整张表被填完。 如果一个没有填的格子所有的数字都不可行，代表之前填的数字有错，那么返回false。 isValid：验证某个格子里的数字是否可行，是之前验证数独的简化版，因为只用验证特定位置的数字。 1234567891011121314151617181920212223242526272829303132333435363738394041class Solution &#123;public: void solveSudoku(vector&lt;vector&lt;char&gt;&gt;&amp; board) &#123; dfs(board, 0, 0); &#125; bool dfs(vector&lt;vector&lt;char&gt;&gt;&amp; board, int i, int j)&#123; if (i&gt;=9) return true; if (j&gt;=9) return dfs(board, i+1, 0); if (board[i][j]=='.')&#123; for (int num = 1; num &lt;= 9; num++)&#123; board[i][j] = (char)(num+'0'); if(isValid(board, i, j))&#123; if(dfs(board, i, j+1)) return true; &#125; board[i][j] = '.'; &#125; &#125;else &#123; return dfs(board, i, j+1); &#125; return false; &#125; bool isValid(vector&lt;vector&lt;char&gt;&gt;&amp; board, int i, int j)&#123; for (int col = 0; col &lt; 9; col++)&#123; if (col != j &amp;&amp; board[i][j]==board[i][col]) return false; &#125; for (int row = 0; row &lt; 9; row++)&#123; if (row != i &amp;&amp; board[i][j]==board[row][j]) return false; &#125; for (int row = i / 3 * 3; row &lt; i / 3 * 3 + 3; ++row) &#123; for (int col = j / 3 * 3; col &lt; j / 3 * 3 + 3; ++col) &#123; if ((row != i || col != j) &amp;&amp; board[i][j] == board[row][col]) return false; &#125; &#125; return true; &#125;&#125;; Reference http://www.cnblogs.com/grandyang/p/4421852.html]]></content>
      <categories>
        <category>coding</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode-36 Valid Sudoku]]></title>
    <url>%2Fcoding%2FValid-Sudoku%2F</url>
    <content type="text"><![CDATA[36. Valid Sudoku要验证数独的棋盘，就是三个标准，行没有重复，列没有重复以及3x3格子没有重复。最开始的思路是建立1*9的hash表，依据这个标准检测三次，但这样带来一个问题是需要遍历棋盘三次。但其实可以扩大hash表，只需遍历棋盘一次。 123456789101112131415161718192021222324class Solution &#123;public: bool row[9][9] = &#123;0&#125;; // 9行的hash bool col[9][9] = &#123;0&#125;; // 9列的hash bool box[9][9] = &#123;0&#125;; // 9个box的hash bool isValidSudoku(vector&lt;vector&lt;char&gt;&gt;&amp; board) &#123; for (int i = 0; i &lt; 9; i++)&#123; for (int j = 0; j &lt; 9; j++)&#123; char c = board[i][j]; if (c != '.')&#123; if(!row[i][c-'1']) row[i][c-'1'] = !row[i][c-'1']; else return false; if(!col[j][c-'1']) col[j][c-'1'] = !col[j][c-'1']; else return false; int idxBox = (i/3)*3+j/3; // 判定该元素属于哪个格子 if(!box[idxBox][c-'1']) box[idxBox][c-'1'] = !box[idxBox][c-'1']; else return false; &#125; &#125; &#125; return true; &#125;&#125;;]]></content>
      <categories>
        <category>coding</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Funcategorized%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[Not me]]></title>
    <url>%2Fabout%2Findex.html</url>
    <content type="text"><![CDATA[棘手之处，耐得住烦 游褒禅山记王安石 &emsp;&emsp;褒禅山亦谓之华山，唐浮图慧褒始舍于其址，而卒葬之；以故其后名之曰“褒禅”。今所谓慧空禅院者，褒之庐冢也。距其院东五里，所谓华山洞者，以其乃华山之阳名之也。距洞百余步，有碑仆道，其文漫灭，独其为文犹可识曰“花山”。今言“华”如“华实”之“华”者，盖音谬也。 &emsp;&emsp;其下平旷，有泉侧出，而记游者甚众，所谓前洞也。由山以上五六里，有穴窈然，入之甚寒，问其深，则其好游者不能穷也，谓之后洞。余与四人拥火以入，入之愈深，其进愈难，而其见愈奇。有怠而欲出者，曰：“不出，火且尽。”遂与之俱出。盖余所至，比好游者尚不能十一，然视其左右，来而记之者已少。盖其又深，则其至又加少矣。方是时，予之力尚足以入，火尚足以明也。既其出，则或咎其欲出者，而余亦悔其随之而不得极夫游之乐也。 &emsp;&emsp;于是余有叹焉。古人之观于天地、山川、草木、虫鱼、鸟兽，往往有得，以其求思之深而无不在也。夫夷以近，则游者众；险以远，则至者少。而世之奇伟、瑰怪，非常之观，常在于险远，而人之所罕至焉，故非有志者不能至也。有志矣，不随以止也，然力不足者，亦不能至也。有志与力，而又不随以怠，至于幽暗昏惑而无物以相之，亦不能至也。然力足以至焉，于人为可讥，而在己为有悔；尽吾志也而不能至者，可以无悔矣，其孰能讥之乎？此余之所得也！ &emsp;&emsp;余于仆碑，又以悲夫古书之不存，后世之谬其传而莫能名者，何可胜道也哉！此所以学者不可以不深思而慎取之也。 &emsp;&emsp;四人者：庐陵萧君圭君玉，长乐王回深父，余弟安国平父、安上纯父。至和元年七月某日，临川王某记。]]></content>
  </entry>
  <entry>
    <title><![CDATA[categories]]></title>
    <url>%2Fcategories%2Findex.html</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Projects]]></title>
    <url>%2Fprojects%2Findex.html</url>
    <content type="text"><![CDATA[ProjectsRoom acoustics 3D sound propagation simulator Audio basics 通过librosa进行音频的基本操作和特征提取 使用librosa库，读取音频，提取频谱，MFCC等。 librosa进行音频重采样 MATLAB中进行声场可视化 matlab中fir设计于滤波 Have fun Urban Sound Classification Inspired by http://aqibsaeed.github.io/2016-09-03-urban-sound-classification-part-1/ Urban Sound Classifier using NN 这是我搭建的第一个神经网络，可以说是边做边学，代码有一些混乱。整个流程几乎是跟着上面博客的思路来，准确度虽然不高，但work了，还是满意的。 Urban Sound Classifier using CNN v2 第一版的CNN非常惨烈，所以废弃了，第二版开始work了，但准确率实在堪忧，只有50%左右。 Urban Sound Classifier using CNN v3 紧接着第二版做的一些变体，单纯的娱乐，看看不同网络效果的区别，虽然没有看到。 Urban Sound Classifier using LSTM v1 第一次尝试LSTM，LSTM的搭建也很简单，预计效果不错，然而实际却低于NN和CNN。原因还不明。 ← Back to blog]]></content>
  </entry>
  <entry>
    <title><![CDATA[tags]]></title>
    <url>%2Ftags%2Findex.html</url>
    <content type="text"></content>
  </entry>
</search>
